{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6eef472-b9ad-4bad-b090-26a26f190821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\srushti\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\srushti\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\srushti\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\srushti\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\srushti\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\srushti\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\srushti\\appdata\\roaming\\python\\python313\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\srushti\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\srushti\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.15.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\srushti\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\srushti\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0159345-9a12-46a4-92d2-7bdae574d6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "236bdf35-4730-429a-b31c-509d23b1cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfdaabe0-0bdf-42ca-85c2-a08d62c05b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Srushti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ea8cf99-3876-43d4-816b-3f01f22563b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Srushti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a308b64-6b3a-40f3-b82a-7926bd42eeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Srushti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3d539158-3973-4d7e-8cff-9716f394951c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Srushti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b328cae-0475-489b-95fd-19a5219a64fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Srushti is learning text analytics. She loves working with data and solving problems.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fe2c3d40-0b50-43ee-9faa-78110020ffcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Srushti', 'is', 'learning', 'text', 'analytics', '.', 'She', 'loves', 'working', 'with', 'data', 'and', 'solving', 'problems', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokens = word_tokenize(doc)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c7e4694-db17-4af8-9cb4-2fddd7e91680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tags: [('Srushti', 'NNP'), ('is', 'VBZ'), ('learning', 'VBG'), ('text', 'JJ'), ('analytics', 'NNS'), ('.', '.'), ('She', 'PRP'), ('loves', 'VBZ'), ('working', 'VBG'), ('with', 'IN'), ('data', 'NNS'), ('and', 'CC'), ('solving', 'VBG'), ('problems', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"POS Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8674d3db-35b6-4335-bb08-10adefe1f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stop Words Removal: ['Srushti', 'learning', 'text', 'analytics', '.', 'loves', 'working', 'data', 'solving', 'problems', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(\"After Stop Words Removal:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a83d46a-8655-46bc-bc54-c029b8ec652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['srushti', 'learn', 'text', 'analyt', '.', 'love', 'work', 'data', 'solv', 'problem', '.']\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8f2aa519-8d8f-415f-87cd-aa661cf89097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['Srushti', 'learning', 'text', 'analytics', '.', 'love', 'working', 'data', 'solving', 'problem', '.']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9eb3813d-4aaf-4444-ac94-5729c8b6f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF & IDF (Term Frequency & Inverse Document Frequency)\n",
    "# corpus is a list or collection of text documents\n",
    "corpus = [\n",
    "    \"Srushti is learning text analytics.\",\n",
    "    \"She loves working with data.\",\n",
    "    \"Analytics helps in solving problems.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3be332a-e0fc-4924-8510-4018a5979782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer is a tool that changes text into numbers (vectors) so machines can work with the text.\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6e9f1350-36fe-4550-8e65-c725cb1fb989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.35543247 0.         0.         0.         0.46735098 0.46735098\n",
      "  0.         0.         0.         0.         0.46735098 0.46735098\n",
      "  0.         0.        ]\n",
      " [0.         0.4472136  0.         0.         0.         0.\n",
      "  0.4472136  0.         0.4472136  0.         0.         0.\n",
      "  0.4472136  0.4472136 ]\n",
      " [0.35543247 0.         0.46735098 0.46735098 0.         0.\n",
      "  0.         0.46735098 0.         0.46735098 0.         0.\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Show TF-IDF scores\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6ffa8d8-42b1-44e5-9143-4a3eafa6f50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Words): ['analytics' 'data' 'helps' 'in' 'is' 'learning' 'loves' 'problems' 'she'\n",
      " 'solving' 'srushti' 'text' 'with' 'working']\n"
     ]
    }
   ],
   "source": [
    "# returns the list of all unique words (features) that the vectorizer has learned from your text data.\n",
    "print(\"Feature Names (Words):\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e3970-218b-45b0-8276-b0c1fbf812e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
